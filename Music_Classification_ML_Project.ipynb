{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Packages Used\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from glob import glob\n",
        "import librosa.display\n",
        "import pandas as pd\n",
        "import torch\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torch.utils.data import Subset\n",
        "import copy\n",
        "import torchvision\n",
        "import itertools\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as trans\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import random_split,SubsetRandomSampler\n",
        "from torchvision.transforms import ToTensor\n",
        "import librosa\n",
        "import time\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import sys"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "hokeGpN0oJ7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The below function is used to get the audio files and convert them to Mel Spectrograms\n",
        "import matplotlib.image\n",
        "dataset_path = \"/kaggle/input/music-genre-data/Data/genres_original\"\n",
        "def convert_to_image():\n",
        "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
        "\n",
        "        if dirpath is not dataset_path:\n",
        "\n",
        "            # save genre label (i.e., sub-folder name) in the mapping\n",
        "            semantic_label = dirpath.split(\"/\")[-1]\n",
        "\n",
        "            if not os.path.exists('/kaggle/working/'+semantic_label):\n",
        "                os.mkdir('/kaggle/working/'+semantic_label)\n",
        "\n",
        "            # process all audio files in genre sub-dir\n",
        "\n",
        "            for i,f in enumerate(filenames):\n",
        "\n",
        "                file_path = os.path.join(dirpath, f)\n",
        "                #print(file_path)\n",
        "                signal, _ = librosa.load(path=file_path)\n",
        "                S = librosa.feature.melspectrogram(y=signal, sr=22050)\n",
        "                S_DB = librosa.power_to_db(S, ref=np.max)\n",
        "                matplotlib.image.imsave('/kaggle/working/'+semantic_label+\"1\"+'/'+str(i)+'image.png', S_DB)\n",
        "            print(semantic_label+\"completed\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "aiWTUpxmoJ7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_to_image()"
      ],
      "metadata": {
        "id": "Z6URWqMAoJ7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The below code gets the images from the directory and converts them into a 224 by 224 normalized tensors.\n",
        "dataset = ImageFolder('/kaggle/input/music-genre-data/Data/images_original', trans.Compose([\n",
        "        trans.Resize((224, 224)),\n",
        "        trans.ToTensor(),\n",
        "        trans.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "]))"
      ],
      "metadata": {
        "trusted": true,
        "id": "M9tUckA6oJ7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_pct = 0.01\n",
        "val_size = int(val_pct * len(dataset))\n",
        "train_ds, valid_ds = random_split(dataset, [len(dataset) - val_size, val_size])\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "e96uLWuBoJ70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the validation and training batch sizes and load them into dataloaders\n",
        "train_batch_size = 250\n",
        "valid_batch_size = 32\n",
        "trainloader = DataLoader(train_ds, batch_size=train_batch_size)\n",
        "validloader = DataLoader(valid_ds, batch_size=valid_batch_size)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Lub_MycBoJ70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The below code gets the pretrained weights and loads them into an object\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_ft = models.vgg16(pretrained=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "f-bGipvmoJ71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "total_trainable_params = sum(\n",
        "    p.numel() for p in model_ft.parameters() if p.requires_grad)\n",
        "\n",
        "total_trainable_params"
      ],
      "metadata": {
        "trusted": true,
        "id": "zJ7SJJ-EoJ71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the below code feezes the parameters of the model since we do not need all parameters for training purpose\n",
        "for param in model_ft.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "trusted": true,
        "id": "fMlWUtGkoJ72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This defines the architectural changes made to our VGG 16 model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_ft = models.vgg16(pretrained=True)\n",
        "\n",
        "num_features = model_ft.classifier[6].in_features\n",
        "features = list(model_ft.classifier.children())[:-1]\n",
        "features.extend([nn.Linear(num_features,2048),\n",
        "                 nn.ReLU(inplace=True),\n",
        "                 nn.Linear(2048,512),\n",
        "                 nn.ReLU(inplace=True),\n",
        "                 nn.Linear(512,256),\n",
        "                 nn.ReLU(inplace=True),\n",
        "                 nn.Linear(256,128),\n",
        "                 nn.ReLU(inplace=True),\n",
        "                 nn.Linear(128,64),\n",
        "                 nn.ReLU(inplace=True),\n",
        "                 nn.Linear(64,10),\n",
        "                      nn.Softmax()])\n",
        "model_ft.classifier = torch.nn.Sequential(*features)\n",
        "print(model_ft)"
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "trusted": true,
        "id": "lusLmby1oJ74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Below lines define the loss function and the optimizer to be used during training.\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)"
      ],
      "metadata": {
        "trusted": true,
        "id": "TKCeDaq4oJ75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# below function counts the total parameters we are going to train our model on.\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(count_parameters(model_ft))"
      ],
      "metadata": {
        "trusted": true,
        "id": "ihnva4jWoJ76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Below function defines the main training loop\n",
        "# It takes as parameters the model , loss, optimizer and number of epochs.\n",
        "def train_model(model, criterion, optimizer, num_epochs):\n",
        "    since = time.time()\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        print(\"-\" * 10)\n",
        "\n",
        "        running_loss = 0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for i,data in enumerate(trainloader):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            inputs,labels=data\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_loss = running_loss / len(trainloader.dataset)\n",
        "        print(f\"Train Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f\"Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "5KAEUyqCoJ77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_based = train_model(model_ft, criterion, optimizer_ft, num_epochs=3)"
      ],
      "metadata": {
        "trusted": true,
        "id": "UP_-PxzjoJ78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# below function calculates the accuracy on the test dataset.\n",
        "def test_accuracy(model,dataloader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    y_preds=[]\n",
        "    y_true=[]\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            images, labels = data\n",
        "            # calculate outputs by running images through the network\n",
        "            outputs = model(images)\n",
        "            # the class with the highest energy is what we choose as prediction\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "            y_preds.extend(predicted)\n",
        "            y_true.extend(labels)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the network on the {total} test Music Samples: {accuracy:.2f} %')\n",
        "    return y_preds,y_true"
      ],
      "metadata": {
        "trusted": true,
        "id": "etAnsr7voJ78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds,y_true=test_accuracy(vgg_based,validloader)"
      ],
      "metadata": {
        "trusted": true,
        "id": "F_6ITqW9oJ79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes=['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop','reggae', 'rock']\n",
        "def confusionmatrix(y_true,y_pred):\n",
        "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],\n",
        "                         columns = [i for i in classes])\n",
        "    plt.figure(figsize = (12,7))\n",
        "    sns.heatmap(df_cm, annot=True)\n",
        "    # plt.savefig('output.png')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-06T14:09:01.401960Z",
          "iopub.execute_input": "2023-05-06T14:09:01.402490Z",
          "iopub.status.idle": "2023-05-06T14:09:01.416122Z",
          "shell.execute_reply.started": "2023-05-06T14:09:01.402390Z",
          "shell.execute_reply": "2023-05-06T14:09:01.414703Z"
        },
        "trusted": true,
        "id": "MSsSub7voJ79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(vgg_based.state_dict(), '/kaggle/working/model.pth')"
      ],
      "metadata": {
        "trusted": true,
        "id": "kcpX8YiXoJ7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r model.zip /kaggle/working"
      ],
      "metadata": {
        "trusted": true,
        "id": "fOSxpQILoJ7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusionmatrix(y_true,y_preds)"
      ],
      "metadata": {
        "trusted": true,
        "id": "0tqBYSDpoJ7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oaESK7NwoJ7-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}